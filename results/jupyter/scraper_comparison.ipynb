{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d390c596-5574-4d48-ab5a-6ac78b69d5cf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# All Scraper Comparison with Domain-Based Processing\n",
    "# Standardized version with consistent evaluation across all scrapers\n",
    "# Domain-based evaluation with weighted scoring: 1 point for single indicator, 2 points for both\n",
    "\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from urllib.parse import urlparse\n",
    "from IPython.display import HTML, display\n",
    "import os\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg\n",
    "from PIL import Image\n",
    "import io\n",
    "import base64\n",
    "import re\n",
    "\n",
    "# Set matplotlib backend and font configuration\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans', 'Liberation Sans', 'Helvetica']\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "MONGO_HOST = 'mongo'\n",
    "MONGO_PORT = 27017\n",
    "MONGO_USER = 'admin'\n",
    "MONGO_PASS = 'changeme'\n",
    "MONGO_DB_NAME = 'tasks'\n",
    "COLLECTIONS_TO_ANALYZE = ['network_scraper', 'api_scraper', 'term_scraper']\n",
    "\n",
    "# --- TASK SELECTION ---\n",
    "TARGET_TASK_NAMES = []  # Empty means analyze all tasks\n",
    "\n",
    "def get_main_domain_from_url(url):\n",
    "    \"\"\"Extract main domain from URL (consistent with scrapers)\"\"\"\n",
    "    try:\n",
    "        parsed_url = urlparse(url)\n",
    "        netloc = parsed_url.netloc\n",
    "        \n",
    "        subdomains_to_remove = [\"sso.\", \"idp.\", \"login.\", \"www.\"]\n",
    "        for subdomain in subdomains_to_remove:\n",
    "            if netloc.startswith(subdomain):\n",
    "                netloc = netloc[len(subdomain):]\n",
    "        \n",
    "        parts = netloc.split(\".\")\n",
    "        if len(parts) > 2:\n",
    "            netloc = \".\".join(parts[-2:])\n",
    "        \n",
    "        return f\"{parsed_url.scheme}://{netloc}\"\n",
    "    except Exception:\n",
    "        return url\n",
    "\n",
    "def detect_secure_specific_indicators(url_data, collection_name):\n",
    "    \"\"\"\n",
    "    Detect SECURE SPECIFIC indicators (API calls, network requests with clear passkey patterns)\n",
    "    These are the most reliable passkey indicators specific to each scraper type.\n",
    "    \n",
    "    Returns True if domain has secure specific indicators, False otherwise.\n",
    "    \"\"\"\n",
    "    if collection_name == 'api_scraper':\n",
    "        # Secure specific: API calls with publicKey\n",
    "        return (url_data.get('api_calls_public_key') and \n",
    "                isinstance(url_data['api_calls_public_key'], list) and \n",
    "                len(url_data['api_calls_public_key']) > 0)\n",
    "    \n",
    "    elif collection_name == 'term_scraper':\n",
    "        # Secure specific: Terms with publicKey parameter\n",
    "        return (url_data.get('navigator_credentials_get({publickey_', False) or\n",
    "                url_data.get('navigator_credentials_create({publickey_', False))\n",
    "    \n",
    "    elif collection_name == 'network_scraper':\n",
    "        # Secure specific: Secure network requests/responses\n",
    "        secure_requests = url_data.get('secure_passkey_requests', [])\n",
    "        secure_responses = url_data.get('secure_passkey_responses', [])\n",
    "        return ((isinstance(secure_requests, list) and len(secure_requests) > 0) or\n",
    "                (isinstance(secure_responses, list) and len(secure_responses) > 0))\n",
    "    \n",
    "    return False\n",
    "\n",
    "def detect_secure_html_indicators(url_data, collection_name):\n",
    "    \"\"\"\n",
    "    Detect SECURE HTML indicators (HTML elements found on page)\n",
    "    These are consistent across all scrapers: webauthn inputs and passkey buttons.\n",
    "    \n",
    "    Returns True if domain has secure HTML indicators, False otherwise.\n",
    "    \"\"\"\n",
    "    return (url_data.get('webauthn_input_found', False) or \n",
    "            url_data.get('passkey_button_found', False))\n",
    "\n",
    "def detect_possible_indicators(url_data, collection_name):\n",
    "    \"\"\"\n",
    "    Detect POSSIBLE indicators (less reliable indicators that might indicate passkey support)\n",
    "    These vary by scraper type but represent uncertain/partial evidence.\n",
    "    \n",
    "    Returns True if domain has possible indicators, False otherwise.\n",
    "    \"\"\"\n",
    "    if collection_name == 'api_scraper':\n",
    "        # Possible: General credential API calls or identity calls\n",
    "        api_calls_cred = url_data.get('api_calls_credentials_get', [])\n",
    "        api_calls_id = url_data.get('api_calls_identity', [])\n",
    "        return ((isinstance(api_calls_cred, list) and len(api_calls_cred) > 0) or\n",
    "                (isinstance(api_calls_id, list) and len(api_calls_id) > 0))\n",
    "    \n",
    "    elif collection_name == 'term_scraper':\n",
    "        # Possible: General navigator.credentials calls without publicKey\n",
    "        return (url_data.get('navigator_credentials_get(', False) or\n",
    "                url_data.get('navigator_credentials_create(', False) or\n",
    "                url_data.get('autocomplete_webauthn', False) or\n",
    "                url_data.get('startauthentication', False) or\n",
    "                url_data.get('isuserverifyingplatformauthenticatoravailable', False))\n",
    "    \n",
    "    elif collection_name == 'network_scraper':\n",
    "        # Possible: Non-secure network patterns\n",
    "        possible_req = url_data.get('possible_passkey_requests', [])\n",
    "        possible_resp = url_data.get('possible_passkey_responses', [])\n",
    "        passkey_req = url_data.get('passkey_requests', [])\n",
    "        passkey_resp = url_data.get('passkey_responses', [])\n",
    "        patterns = url_data.get('passkey_patterns_detected', False)\n",
    "        \n",
    "        return ((isinstance(possible_req, list) and len(possible_req) > 0) or\n",
    "                (isinstance(possible_resp, list) and len(possible_resp) > 0) or\n",
    "                (isinstance(passkey_req, list) and len(passkey_req) > 0) or\n",
    "                (isinstance(passkey_resp, list) and len(passkey_resp) > 0) or\n",
    "                patterns)\n",
    "    \n",
    "    return False\n",
    "\n",
    "def detect_fedcm_indicators(url_data, collection_name):\n",
    "    \"\"\"\n",
    "    Detect FedCM (Federated Credential Management) indicators\n",
    "    These are specific to identity/federated authentication.\n",
    "    \n",
    "    Returns True if domain has FedCM indicators, False otherwise.\n",
    "    \"\"\"\n",
    "    if collection_name == 'api_scraper':\n",
    "        # FedCM: Identity API calls\n",
    "        api_calls_id = url_data.get('api_calls_identity', [])\n",
    "        return (isinstance(api_calls_id, list) and len(api_calls_id) > 0)\n",
    "    \n",
    "    elif collection_name == 'term_scraper':\n",
    "        # FedCM: Terms with identity or federated parameters\n",
    "        return (url_data.get('navigator_credentials_get({identity_', False) or\n",
    "                url_data.get('navigator_credentials_create({identity_', False) or\n",
    "                url_data.get('navigator_credentials_get({federated_', False) or\n",
    "                url_data.get('navigator_credentials_create({federated_', False))\n",
    "    \n",
    "    elif collection_name == 'network_scraper':\n",
    "        # FedCM: Network FedCM detections\n",
    "        fedcm_detections = url_data.get('fedcm_detections', [])\n",
    "        return (isinstance(fedcm_detections, list) and len(fedcm_detections) > 0)\n",
    "    \n",
    "    return False\n",
    "\n",
    "def evaluate_domain_classification(domain_data, collection_name):\n",
    "    \"\"\"\n",
    "    STANDARDIZED domain classification logic used across all scrapers.\n",
    "    Each domain is classified into exactly ONE category based on priority:\n",
    "    1. Secure Both (HTML + Specific) - highest priority\n",
    "    2. Secure HTML Only\n",
    "    3. Secure Specific Only  \n",
    "    4. No Secure Indicators Found - lowest priority\n",
    "    \n",
    "    Args:\n",
    "        domain_data: List of URL data for the domain\n",
    "        collection_name: Name of the collection (scraper type)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Classification results with flags and indicators found\n",
    "    \"\"\"\n",
    "    # Check all URLs in domain for any indicators\n",
    "    has_secure_specific = False\n",
    "    has_secure_html = False\n",
    "    has_possible = False  \n",
    "    has_fedcm = False\n",
    "    \n",
    "    for url_data in domain_data:\n",
    "        if detect_secure_specific_indicators(url_data, collection_name):\n",
    "            has_secure_specific = True\n",
    "        if detect_secure_html_indicators(url_data, collection_name):  \n",
    "            has_secure_html = True\n",
    "        if detect_possible_indicators(url_data, collection_name):\n",
    "            has_possible = True\n",
    "        if detect_fedcm_indicators(url_data, collection_name):\n",
    "            has_fedcm = True\n",
    "    \n",
    "    # Classify domain based on secure indicators (HTML + Specific combination)\n",
    "    classification = {\n",
    "        'secure_html_only': False,\n",
    "        'secure_specific_only': False, \n",
    "        'secure_both': False,\n",
    "        'no_secure_found': False,\n",
    "        'has_possible': has_possible,\n",
    "        'has_fedcm': has_fedcm,\n",
    "        'weighted_score': 0  # 1 point for single, 2 points for both\n",
    "    }\n",
    "    \n",
    "    if has_secure_html and has_secure_specific:\n",
    "        classification['secure_both'] = True\n",
    "        classification['weighted_score'] = 2\n",
    "    elif has_secure_html:\n",
    "        classification['secure_html_only'] = True  \n",
    "        classification['weighted_score'] = 1\n",
    "    elif has_secure_specific:\n",
    "        classification['secure_specific_only'] = True\n",
    "        classification['weighted_score'] = 1\n",
    "    else:\n",
    "        classification['no_secure_found'] = True\n",
    "        classification['weighted_score'] = 0\n",
    "    \n",
    "    return classification\n",
    "\n",
    "def load_and_process_collection_data(db, collection_name, target_tasks):\n",
    "    \"\"\"Load data from a collection and process it for domain-based analysis\"\"\"\n",
    "    collection = db[collection_name]\n",
    "    \n",
    "    # Build base query\n",
    "    base_query = {}\n",
    "    if target_tasks:\n",
    "        base_query['result.task_name'] = {'$in': target_tasks}\n",
    "    \n",
    "    data = list(collection.find(base_query))\n",
    "    if not data:\n",
    "        print(f\"No documents found in '{collection_name}'.\")\n",
    "        return pd.DataFrame(), {}\n",
    "\n",
    "    normalized_data = []\n",
    "    for doc in data:\n",
    "        if 'result' in doc and doc['result'] is not None:\n",
    "            if isinstance(doc['result'], dict):\n",
    "                flat_doc = pd.json_normalize(doc['result']).to_dict(orient='records')[0]\n",
    "                \n",
    "                # Handle NaN values in boolean columns\n",
    "                bool_columns = (\n",
    "                    ['webauthn_input_found', 'passkey_button_found', 'error'] +\n",
    "                    [col for col in flat_doc.keys() if isinstance(col, str) and col.endswith('_found')]\n",
    "                )\n",
    "                for bool_col in bool_columns:\n",
    "                    if bool_col in flat_doc and pd.isna(flat_doc[bool_col]):\n",
    "                        flat_doc[bool_col] = False\n",
    "                \n",
    "                normalized_data.append(flat_doc)\n",
    "    \n",
    "    df = pd.DataFrame(normalized_data)\n",
    "    if df.empty:\n",
    "        return df, {}\n",
    "    \n",
    "    print(f\"{len(df)} documents loaded from '{collection_name}'.\")\n",
    "    \n",
    "    # Group by domain (url_id)\n",
    "    domain_groups = defaultdict(list)\n",
    "    for _, row in df.iterrows():\n",
    "        domain = row.get('url_id', 'unknown')\n",
    "        domain_groups[domain].append(row)\n",
    "    \n",
    "    return df, domain_groups\n",
    "\n",
    "def count_indicators_for_collection(domain_groups, collection_name):\n",
    "    \"\"\"\n",
    "    STANDARDIZED indicator counting using the new classification logic.\n",
    "    Count domains in each category using consistent evaluation criteria.\n",
    "    \"\"\"\n",
    "    if not domain_groups:\n",
    "        return {\n",
    "            'secure_html_only': 0,\n",
    "            'secure_specific_only': 0,\n",
    "            'secure_both': 0,\n",
    "            'total_secure': 0,\n",
    "            'no_secure_found': 0,\n",
    "            'possible': 0,\n",
    "            'fedcm': 0,\n",
    "            'weighted_total': 0\n",
    "        }\n",
    "    \n",
    "    # Counters for each classification\n",
    "    secure_html_only = 0\n",
    "    secure_specific_only = 0\n",
    "    secure_both = 0\n",
    "    possible_count = 0\n",
    "    fedcm_count = 0\n",
    "    weighted_total = 0\n",
    "    \n",
    "    for domain, domain_data in domain_groups.items():\n",
    "        classification = evaluate_domain_classification(domain_data, collection_name)\n",
    "        \n",
    "        if classification['secure_html_only']:\n",
    "            secure_html_only += 1\n",
    "        elif classification['secure_specific_only']:\n",
    "            secure_specific_only += 1\n",
    "        elif classification['secure_both']:\n",
    "            secure_both += 1\n",
    "            \n",
    "        if classification['has_possible']:\n",
    "            possible_count += 1\n",
    "        if classification['has_fedcm']:\n",
    "            fedcm_count += 1\n",
    "            \n",
    "        weighted_total += classification['weighted_score']\n",
    "    \n",
    "    total_secure = secure_html_only + secure_specific_only + secure_both\n",
    "    no_secure_found = len(domain_groups) - total_secure\n",
    "    \n",
    "    return {\n",
    "        'secure_html_only': secure_html_only,\n",
    "        'secure_specific_only': secure_specific_only,\n",
    "        'secure_both': secure_both,\n",
    "        'total_secure': total_secure,\n",
    "        'no_secure_found': no_secure_found,\n",
    "        'possible': possible_count,\n",
    "        'fedcm': fedcm_count,\n",
    "        'weighted_total': weighted_total\n",
    "    }\n",
    "\n",
    "def get_timeout_compliance_data(db, collections, target_tasks):\n",
    "    \"\"\"Collects and categorizes timeout compliance data for all tasks.\"\"\"\n",
    "    \n",
    "    timeout_data = []\n",
    "    base_query = {'result.task_name': {'$in': target_tasks}} if target_tasks else {}\n",
    "    \n",
    "    for col_name in collections:\n",
    "        collection = db[col_name]\n",
    "        task_names_in_col = collection.distinct('result.task_name', base_query)\n",
    "        \n",
    "        for task_name in task_names_in_col:\n",
    "            task_query = {'result.task_name': task_name}\n",
    "            docs = collection.find(task_query, {'result.duration_seconds': 1, 'result.timeout': 1})\n",
    "            \n",
    "            counts = {\"0-50%\": 0, \"50-100%\": 0, \"100-150%\": 0, \">150%\": 0}\n",
    "            total_docs = 0\n",
    "            timeout_val = None\n",
    "            total_duration = 0\n",
    "            \n",
    "            # Find timeout value for this task\n",
    "            first_doc = collection.find_one(task_query, {'result.timeout': 1})\n",
    "            if first_doc and 'result' in first_doc and 'timeout' in first_doc['result']:\n",
    "                timeout_val = first_doc['result']['timeout']\n",
    "                if timeout_val is not None and timeout_val > 0:\n",
    "                    timeout_val = float(timeout_val)\n",
    "            \n",
    "            if timeout_val:\n",
    "                # Calculate compliance\n",
    "                docs_for_compliance = collection.find(task_query, {'result.duration_seconds': 1})\n",
    "                for doc in docs_for_compliance:\n",
    "                    total_docs += 1\n",
    "                    duration = doc.get('result', {}).get('duration_seconds')\n",
    "                    if duration is not None:\n",
    "                        total_duration += duration\n",
    "                        compliance = duration / timeout_val\n",
    "                        if compliance <= 0.5:\n",
    "                            counts[\"0-50%\"] += 1\n",
    "                        elif compliance <= 1.0:\n",
    "                            counts[\"50-100%\"] += 1\n",
    "                        elif compliance <= 1.5:\n",
    "                            counts[\"100-150%\"] += 1\n",
    "                        else:\n",
    "                            counts[\">150%\"] += 1\n",
    "                \n",
    "                if total_docs > 0:\n",
    "                    # Format duration as hours:minutes:seconds\n",
    "                    hours, remainder = divmod(int(total_duration), 3600)\n",
    "                    minutes, seconds = divmod(remainder, 60)\n",
    "                    duration_formatted = f\"{hours:02d}:{minutes:02d}:{seconds:02d}\"\n",
    "                    \n",
    "                    timeout_data.append({\n",
    "                        'task_label': f\"{task_name}\\n({col_name} / {int(timeout_val)}s)\",\n",
    "                        'total': total_docs,\n",
    "                        'total_duration': total_duration,\n",
    "                        'duration_formatted': duration_formatted,\n",
    "                        **counts\n",
    "                    })\n",
    "    \n",
    "    return timeout_data\n",
    "\n",
    "def analyze_task_based_detection_rates(db, collections, target_tasks):\n",
    "    \"\"\"Analyze detection success rates for each task, showing percentage of domains with secure indicators.\"\"\"\n",
    "    task_detection_rates = []\n",
    "    \n",
    "    for collection_name in collections:\n",
    "        print(f\"🔍 Analyzing task detection rates for {collection_name}...\")\n",
    "        \n",
    "        # FIXED: Use the same data loading logic as the main comparison\n",
    "        df, domain_groups = load_and_process_collection_data(db, collection_name, target_tasks)\n",
    "        \n",
    "        if not domain_groups:\n",
    "            continue\n",
    "            \n",
    "        # Get unique task names from the loaded data\n",
    "        if not df.empty and 'task_name' in df.columns:\n",
    "            task_names = df['task_name'].unique()\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        for task_name in task_names:\n",
    "            # Filter domain groups for this specific task\n",
    "            task_domain_groups = defaultdict(list)\n",
    "            \n",
    "            for domain, domain_data in domain_groups.items():\n",
    "                # Filter URLs for this task\n",
    "                task_urls = [url_data for url_data in domain_data if url_data.get('task_name') == task_name]\n",
    "                if task_urls:\n",
    "                    task_domain_groups[domain] = task_urls\n",
    "            \n",
    "            total_domains = len(task_domain_groups)\n",
    "            if total_domains == 0:\n",
    "                continue\n",
    "            \n",
    "            # FIXED: Use the same standardized counting logic as the main comparison\n",
    "            indicator_counts = count_indicators_for_collection(task_domain_groups, collection_name)\n",
    "            domains_with_indicators_count = indicator_counts['total_secure']\n",
    "            \n",
    "            # Calculate success rate\n",
    "            success_rate = (domains_with_indicators_count / total_domains) * 100\n",
    "            \n",
    "            print(f\"   • Task: {task_name}\")\n",
    "            print(f\"     - Total domains: {total_domains}\")\n",
    "            print(f\"     - Domains with secure indicators: {domains_with_indicators_count}\")\n",
    "            print(f\"     - Success rate: {success_rate:.1f}%\")\n",
    "            \n",
    "            task_detection_rates.append({\n",
    "                'collection': collection_name,\n",
    "                'task_name': task_name,\n",
    "                'total_domains': total_domains,\n",
    "                'domains_with_indicators': domains_with_indicators_count,\n",
    "                'success_rate': success_rate\n",
    "            })\n",
    "    \n",
    "    return task_detection_rates\n",
    "\n",
    "def save_figure_as_png(fig, filename, dpi=300):\n",
    "    \"\"\"Save a matplotlib figure as a PNG file with high resolution\"\"\"\n",
    "    fig.savefig(filename, bbox_inches='tight', dpi=dpi)\n",
    "    print(f\"Saved: {filename}\")\n",
    "\n",
    "def create_comprehensive_visualization(comparison_data, timeout_data, task_detection_rates):\n",
    "    \"\"\"Create the complete visualization with 4 charts including weighted scores\"\"\"\n",
    "    # Create the main figure with all charts\n",
    "    fig = plt.figure(figsize=(24, 18))\n",
    "    \n",
    "    # Chart 1: Domain-based Comparison with Enhanced Stacked Bar for Secure Indicators + Weighted Scores\n",
    "    ax1 = plt.subplot(2, 2, 1)\n",
    "    collections = list(comparison_data.keys())\n",
    "    \n",
    "    # Extract data for the visualization\n",
    "    secure_html_counts = [comparison_data[col]['secure_html_only'] for col in collections]\n",
    "    secure_specific_counts = [comparison_data[col]['secure_specific_only'] for col in collections]\n",
    "    secure_both_counts = [comparison_data[col]['secure_both'] for col in collections]\n",
    "    possible_counts = [comparison_data[col]['possible_passkey'] for col in collections]\n",
    "    fedcm_counts = [comparison_data[col]['fedcm'] for col in collections]\n",
    "    weighted_totals = [comparison_data[col]['weighted_total'] for col in collections]\n",
    "    \n",
    "    x = np.arange(len(collections))\n",
    "    width = 0.2\n",
    "    \n",
    "    # Stacked bar for secure indicators with three segments\n",
    "    bars1_html = ax1.bar(x - width, secure_html_counts, width, \n",
    "                         label='HTML Elements Only (1pt)', \n",
    "                         color='#3498db', alpha=0.7, edgecolor='white', linewidth=1)\n",
    "    \n",
    "    bars1_specific = ax1.bar(x - width, secure_specific_counts, width, \n",
    "                            bottom=secure_html_counts,\n",
    "                            label='Specific Indicators Only (1pt)',\n",
    "                            color='#2ecc71', alpha=0.7, edgecolor='white', linewidth=1)\n",
    "    \n",
    "    # Calculate bottom position for the third segment (both)\n",
    "    bottom_both = [h + s for h, s in zip(secure_html_counts, secure_specific_counts)]\n",
    "    \n",
    "    bars1_both = ax1.bar(x - width, secure_both_counts, width,\n",
    "                        bottom=bottom_both,\n",
    "                        label='Both (HTML + Specific) (2pts)',\n",
    "                        color='#9b59b6', alpha=0.7, edgecolor='white', linewidth=1)\n",
    "    \n",
    "    bars2 = ax1.bar(x, possible_counts, width, \n",
    "                   label='Possible Indicators',\n",
    "                   color='orange', alpha=0.7, edgecolor='white', linewidth=1)\n",
    "    \n",
    "    bars3 = ax1.bar(x + width, fedcm_counts, width, \n",
    "                   label='FedCM Indicators',\n",
    "                   color='blue', alpha=0.7, edgecolor='white', linewidth=1)\n",
    "    \n",
    "    ax1.set_xlabel('Scraper Type')\n",
    "    ax1.set_ylabel('Number of Domains')\n",
    "    ax1.set_title('Standardized Domain-Based Comparison: Secure vs Possible Indicators')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels([col.replace('_', ' ').title() for col in collections])\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels and weighted scores\n",
    "    for i, collection in enumerate(collections):\n",
    "        # HTML only segment\n",
    "        if secure_html_counts[i] > 0:\n",
    "            height = secure_html_counts[i] / 2\n",
    "            ax1.text(i - width, height, str(secure_html_counts[i]), \n",
    "                    ha='center', va='center', fontweight='bold', \n",
    "                    color='white' if secure_html_counts[i] > 2 else 'black')\n",
    "        \n",
    "        # Specific indicators segment\n",
    "        if secure_specific_counts[i] > 0:\n",
    "            height = secure_html_counts[i] + secure_specific_counts[i] / 2\n",
    "            ax1.text(i - width, height, str(secure_specific_counts[i]), \n",
    "                    ha='center', va='center', fontweight='bold', \n",
    "                    color='white' if secure_specific_counts[i] > 2 else 'black')\n",
    "        \n",
    "        # Both indicators segment\n",
    "        if secure_both_counts[i] > 0:\n",
    "            height = secure_html_counts[i] + secure_specific_counts[i] + secure_both_counts[i] / 2\n",
    "            ax1.text(i - width, height, str(secure_both_counts[i]), \n",
    "                    ha='center', va='center', fontweight='bold', \n",
    "                    color='white' if secure_both_counts[i] > 2 else 'black')\n",
    "        \n",
    "        # Total count with weighted score\n",
    "        total_secure = secure_html_counts[i] + secure_specific_counts[i] + secure_both_counts[i]\n",
    "        if total_secure > 0:\n",
    "            ax1.text(i - width, total_secure + 0.1, f'{total_secure}\\n({weighted_totals[i]}pts)', \n",
    "                    ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "        \n",
    "        # Other indicators\n",
    "        if possible_counts[i] > 0:\n",
    "            ax1.text(i, possible_counts[i] + 0.1, str(possible_counts[i]), \n",
    "                    ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        if fedcm_counts[i] > 0:\n",
    "            ax1.text(i + width, fedcm_counts[i] + 0.1, str(fedcm_counts[i]), \n",
    "                    ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Chart 2: Total Domains vs URLs\n",
    "    ax2 = plt.subplot(2, 2, 2)\n",
    "    total_domains = [comparison_data[col]['total_domains'] for col in collections]\n",
    "    total_urls = [comparison_data[col]['total_urls'] for col in collections]\n",
    "    \n",
    "    x2 = np.arange(len(collections))\n",
    "    width2 = 0.35\n",
    "    \n",
    "    ax2.bar(x2 - width2/2, total_domains, width2, label='Unique Domains', color='skyblue', alpha=0.7)\n",
    "    ax2.bar(x2 + width2/2, total_urls, width2, label='Total URLs', color='lightcoral', alpha=0.7)\n",
    "    \n",
    "    ax2.set_xlabel('Scraper Type')\n",
    "    ax2.set_ylabel('Count')\n",
    "    ax2.set_title('Domains vs URLs per Scraper')\n",
    "    ax2.set_xticks(x2)\n",
    "    ax2.set_xticklabels([col.replace('_', ' ').title() for col in collections])\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    for i, (domains, urls) in enumerate(zip(total_domains, total_urls)):\n",
    "        ax2.text(i - width2/2, domains + 1, str(domains), ha='center', va='bottom', fontweight='bold')\n",
    "        ax2.text(i + width2/2, urls + 1, str(urls), ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Chart 3: Task Detection Success Rate\n",
    "    ax3 = plt.subplot(2, 2, 3)\n",
    "    \n",
    "    if not task_detection_rates or len(task_detection_rates) == 0:\n",
    "        ax3.text(0.5, 0.5, 'No task data available', ha='center', va='center', transform=ax3.transAxes)\n",
    "        ax3.set_title('Task-Based Detection Success Rate')\n",
    "    else:\n",
    "        # Sort by success rate for better visualization\n",
    "        task_detection_rates.sort(key=lambda x: x['success_rate'], reverse=True)\n",
    "        \n",
    "        # Limit to top 15 tasks for readability\n",
    "        show_tasks = task_detection_rates[:15]\n",
    "        \n",
    "        tasks = [f\"{item['task_name']} ({item['collection']})\" for item in show_tasks]\n",
    "        success_rates = [item['success_rate'] for item in show_tasks]\n",
    "        colors = []\n",
    "        \n",
    "        # Assign colors based on collection\n",
    "        for item in show_tasks:\n",
    "            if item['collection'] == 'network_scraper':\n",
    "                colors.append('#3498db')\n",
    "            elif item['collection'] == 'api_scraper':\n",
    "                colors.append('#2ecc71')\n",
    "            else:  # term_scraper\n",
    "                colors.append('#9b59b6')\n",
    "        \n",
    "        # Create horizontal bar chart\n",
    "        y_pos = np.arange(len(tasks))\n",
    "        bars = ax3.barh(y_pos, success_rates, align='center', color=colors, alpha=0.7)\n",
    "        \n",
    "        ax3.set_yticks(y_pos)\n",
    "        ax3.set_yticklabels(tasks)\n",
    "        ax3.set_xlabel('Success Rate (%)')\n",
    "        ax3.set_title('Task-Based Detection Success Rate (% of Domains with Secure Indicators)')\n",
    "        ax3.set_xlim(0, 100)\n",
    "        ax3.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, bar in enumerate(bars):\n",
    "            width = bar.get_width()\n",
    "            indicator_count = show_tasks[i]['domains_with_indicators']\n",
    "            total_count = show_tasks[i]['total_domains']\n",
    "            ax3.text(width + 1, bar.get_y() + bar.get_height()/2, \n",
    "                   f'{width:.1f}% ({indicator_count}/{total_count})', \n",
    "                   ha='left', va='center', fontweight='bold')\n",
    "    \n",
    "    # Chart 4: Timeout Compliance\n",
    "    ax4 = plt.subplot(2, 2, 4)\n",
    "    \n",
    "    if timeout_data:\n",
    "        timeout_df = pd.DataFrame(timeout_data)\n",
    "        \n",
    "        enhanced_labels = [f\"{label}\\nTotal: {row['duration_formatted']} ({int(row['total_duration'])}s)\" \n",
    "                          for label, row in zip(timeout_df['task_label'], timeout_df.to_dict('records'))]\n",
    "        \n",
    "        tasks = enhanced_labels\n",
    "        categories = ['0-50%', '50-100%', '100-150%', '>150%']\n",
    "        colors_timeout = ['lightgreen', 'green', 'yellow', 'darkred']\n",
    "        \n",
    "        # Main chart\n",
    "        bottom = np.zeros(len(tasks))\n",
    "        for i, category in enumerate(categories):\n",
    "            values = timeout_df[category].tolist()\n",
    "            ax4.bar(range(len(tasks)), values, bottom=bottom, label=category, \n",
    "                   color=colors_timeout[i], alpha=0.7)\n",
    "            bottom += values\n",
    "        \n",
    "        ax4.set_xlabel('Task (Scraper / Timeout / Total Duration)')\n",
    "        ax4.set_ylabel('Number of URLs')\n",
    "        ax4.set_title('Timeout Compliance Distribution with Total Task Duration')\n",
    "        ax4.set_xticks(range(len(tasks)))\n",
    "        ax4.set_xticklabels(tasks, rotation=45, ha='right')\n",
    "        ax4.legend(title='Timeout Usage')\n",
    "    else:\n",
    "        ax4.text(0.5, 0.5, 'No timeout data available', ha='center', va='center', transform=ax4.transAxes)\n",
    "        ax4.set_title('Timeout Analysis')\n",
    "    \n",
    "    # Display the combined figure\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def calculate_average_duration(df):\n",
    "    \"\"\"Calculate average duration per URL from the dataframe\"\"\"\n",
    "    if df.empty:\n",
    "        return 0.0\n",
    "    \n",
    "    durations = df['duration_seconds'].dropna()\n",
    "    if len(durations) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return durations.mean()\n",
    "\n",
    "def create_summary_table(comparison_data):\n",
    "    \"\"\"Create a summary table of results including weighted scores\"\"\"\n",
    "    table_html = '''<h3>📊 Summary of Standardized Domain-based Results</h3>\n",
    "    <table style=\"border-collapse: collapse; width: 100%; margin: 20px 0;\">\n",
    "        <tr style=\"background-color: #f2f2f2;\">\n",
    "            <th style=\"border: 1px solid #ddd; padding: 12px; text-align: left;\">Scraper</th>\n",
    "            <th style=\"border: 1px solid #ddd; padding: 12px; text-align: center;\">Domains</th>\n",
    "            <th style=\"border: 1px solid #ddd; padding: 12px; text-align: center;\">URLs</th>\n",
    "            <th style=\"border: 1px solid #ddd; padding: 12px; text-align: center;\">Secure Indicators</th>\n",
    "            <th style=\"border: 1px solid #ddd; padding: 12px; text-align: center;\">Weighted Score</th>\n",
    "            <th style=\"border: 1px solid #ddd; padding: 12px; text-align: center;\">Possible</th>\n",
    "            <th style=\"border: 1px solid #ddd; padding: 12px; text-align: center;\">FedCM</th>\n",
    "            <th style=\"border: 1px solid #ddd; padding: 12px; text-align: center;\">Success Rate</th>\n",
    "        </tr>'''\n",
    "    \n",
    "    for scraper, data in comparison_data.items():\n",
    "        # FIXED: Use consistent variable name 'secure_passkey' which contains the total_secure count\n",
    "        success_rate = (data['secure_passkey'] / data['total_domains'] * 100) if data['total_domains'] > 0 else 0\n",
    "        avg_duration = data.get('avg_duration_per_url', 0)\n",
    "        table_html += f'''\n",
    "        <tr>\n",
    "            <td style=\"border: 1px solid #ddd; padding: 12px; font-weight: bold;\">{scraper.replace('_', ' ').title()}</td>\n",
    "            <td style=\"border: 1px solid #ddd; padding: 12px; text-align: center;\">{data['total_domains']}</td>\n",
    "            <td style=\"border: 1px solid #ddd; padding: 12px; text-align: center;\">{data['total_urls']}</td>\n",
    "            <td style=\"border: 1px solid #ddd; padding: 12px; text-align: center; color: green; font-weight: bold;\">{data['secure_passkey']}</td>\n",
    "            <td style=\"border: 1px solid #ddd; padding: 12px; text-align: center; color: purple; font-weight: bold;\">{data['weighted_total']}</td>\n",
    "            <td style=\"border: 1px solid #ddd; padding: 12px; text-align: center; color: orange; font-weight: bold;\">{data['possible_passkey']}</td>\n",
    "            <td style=\"border: 1px solid #ddd; padding: 12px; text-align: center; color: blue; font-weight: bold;\">{data['fedcm']}</td>\n",
    "            <td style=\"border: 1px solid #ddd; padding: 12px; text-align: center; font-weight: bold;\">{success_rate:.1f}%</td>\n",
    "        </tr>'''\n",
    "    \n",
    "    table_html += '</table>'\n",
    "    table_html += '''\n",
    "    <p><strong>Weighted Scoring:</strong></p>\n",
    "    <ul>\n",
    "        <li>HTML Elements Only OR Specific Indicators Only: 1 point</li>\n",
    "        <li>Both HTML Elements AND Specific Indicators: 2 points</li>\n",
    "        <li>No Secure Indicators: 0 points</li>\n",
    "    </ul>\n",
    "    <p><strong>Note:</strong> Success Rate and Task-Based Detection Rate should now be identical when only one task per scraper type exists.</p>\n",
    "    '''\n",
    "    return table_html\n",
    "\n",
    "# ====================== MAIN EXECUTION ======================\n",
    "\n",
    "try:\n",
    "    # Connect to MongoDB\n",
    "    client = pymongo.MongoClient(f'mongodb://{MONGO_USER}:{MONGO_PASS}@{MONGO_HOST}:{MONGO_PORT}/')\n",
    "    db = client[MONGO_DB_NAME]\n",
    "    \n",
    "    print(\"🔍 Loading and processing data from all scrapers...\")\n",
    "    print(\"🎯 Using STANDARDIZED evaluation logic across all scrapers:\")\n",
    "    print(\"   • Secure HTML: webauthn_input_found OR passkey_button_found\")  \n",
    "    print(\"   • Secure Specific: API calls with publicKey / Terms with publicKey / Secure network requests\")\n",
    "    print(\"   • Domain Classification: HTML only (1pt) | Specific only (1pt) | Both (2pts) | None (0pts)\")\n",
    "    \n",
    "    comparison_data = {}\n",
    "    all_domain_groups = {}  # Store for task analysis\n",
    "    \n",
    "    for collection_name in COLLECTIONS_TO_ANALYZE:\n",
    "        print(f\"\\n📊 Processing {collection_name} with standardized logic...\")\n",
    "        \n",
    "        # Load data with domain-based processing\n",
    "        df, domain_groups = load_and_process_collection_data(\n",
    "            db, collection_name, TARGET_TASK_NAMES\n",
    "        )\n",
    "        \n",
    "        # Store for task analysis to ensure consistency\n",
    "        all_domain_groups[collection_name] = (df, domain_groups)\n",
    "        \n",
    "        if domain_groups:\n",
    "            # Count indicators per domain using STANDARDIZED logic\n",
    "            indicator_counts = count_indicators_for_collection(domain_groups, collection_name)\n",
    "            \n",
    "            # Calculate average duration per URL\n",
    "            avg_duration = calculate_average_duration(df)\n",
    "            \n",
    "            comparison_data[collection_name] = {\n",
    "                'total_domains': len(domain_groups),\n",
    "                'total_urls': len(df),\n",
    "                'avg_duration_per_url': avg_duration,\n",
    "                'secure_html_only': indicator_counts['secure_html_only'],\n",
    "                'secure_specific_only': indicator_counts['secure_specific_only'],\n",
    "                'secure_both': indicator_counts['secure_both'],\n",
    "                'secure_passkey': indicator_counts['total_secure'],\n",
    "                'possible_passkey': indicator_counts['possible'],\n",
    "                'fedcm': indicator_counts['fedcm'],\n",
    "                'weighted_total': indicator_counts['weighted_total']\n",
    "            }\n",
    "            \n",
    "            print(f\"   • Domains: {len(domain_groups)}, URLs: {len(df)}\")\n",
    "            print(f\"   • HTML only: {indicator_counts['secure_html_only']} (1pt each)\")\n",
    "            print(f\"   • Specific only: {indicator_counts['secure_specific_only']} (1pt each)\") \n",
    "            print(f\"   • Both: {indicator_counts['secure_both']} (2pts each)\")\n",
    "            print(f\"   • Total secure: {indicator_counts['total_secure']}, Weighted: {indicator_counts['weighted_total']} pts\")\n",
    "            print(f\"   • Possible: {indicator_counts['possible']}, FedCM: {indicator_counts['fedcm']}\")\n",
    "        else:\n",
    "            comparison_data[collection_name] = {\n",
    "                'total_domains': 0, 'total_urls': 0, 'avg_duration_per_url': 0.0,\n",
    "                'secure_html_only': 0, 'secure_specific_only': 0, 'secure_both': 0,\n",
    "                'secure_passkey': 0, 'possible_passkey': 0, 'fedcm': 0, 'weighted_total': 0\n",
    "            }\n",
    "            print(f\"   • No data found\")\n",
    "    \n",
    "    # Get timeout compliance data\n",
    "    print(\"\\n⏱️ Analyzing timeout compliance...\")\n",
    "    timeout_data = get_timeout_compliance_data(db, COLLECTIONS_TO_ANALYZE, TARGET_TASK_NAMES)\n",
    "    \n",
    "    # FIXED: Analyze task-based detection rates using the same loaded data\n",
    "    print(\"\\n📊 Analyzing task-based detection success rates...\")\n",
    "    task_detection_rates = []\n",
    "    \n",
    "    for collection_name, (df, domain_groups) in all_domain_groups.items():\n",
    "        if domain_groups and not df.empty and 'task_name' in df.columns:\n",
    "            print(f\"🔍 Analyzing task detection rates for {collection_name}...\")\n",
    "            \n",
    "            task_names = df['task_name'].unique()\n",
    "            \n",
    "            for task_name in task_names:\n",
    "                # Filter domain groups for this specific task\n",
    "                task_domain_groups = defaultdict(list)\n",
    "                \n",
    "                for domain, domain_data in domain_groups.items():\n",
    "                    # Filter URLs for this task\n",
    "                    task_urls = [url_data for url_data in domain_data if url_data.get('task_name') == task_name]\n",
    "                    if task_urls:\n",
    "                        task_domain_groups[domain] = task_urls\n",
    "                \n",
    "                total_domains = len(task_domain_groups)\n",
    "                if total_domains == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Use the same standardized counting logic\n",
    "                indicator_counts = count_indicators_for_collection(task_domain_groups, collection_name)\n",
    "                domains_with_indicators_count = indicator_counts['total_secure']\n",
    "                \n",
    "                # Calculate success rate\n",
    "                success_rate = (domains_with_indicators_count / total_domains) * 100\n",
    "                \n",
    "                print(f\"   • Task: {task_name}\")\n",
    "                print(f\"     - Total domains: {total_domains}\")\n",
    "                print(f\"     - Domains with secure indicators: {domains_with_indicators_count}\")\n",
    "                print(f\"     - Success rate: {success_rate:.1f}%\")\n",
    "                \n",
    "                # Verify consistency with main analysis\n",
    "                if len(task_names) == 1:  # Only one task per scraper\n",
    "                    main_total = comparison_data[collection_name]['total_domains']\n",
    "                    main_secure = comparison_data[collection_name]['secure_passkey']\n",
    "                    main_rate = (main_secure / main_total * 100) if main_total > 0 else 0\n",
    "                    \n",
    "                    if abs(success_rate - main_rate) > 0.1:\n",
    "                        print(f\"   ⚠️  WARNING: Inconsistency detected!\")\n",
    "                        print(f\"     - Main analysis: {main_secure}/{main_total} = {main_rate:.1f}%\")\n",
    "                        print(f\"     - Task analysis: {domains_with_indicators_count}/{total_domains} = {success_rate:.1f}%\")\n",
    "                \n",
    "                task_detection_rates.append({\n",
    "                    'collection': collection_name,\n",
    "                    'task_name': task_name,\n",
    "                    'total_domains': total_domains,\n",
    "                    'domains_with_indicators': domains_with_indicators_count,\n",
    "                    'success_rate': success_rate\n",
    "                })\n",
    "    \n",
    "    # Create visualizations for display\n",
    "    print(\"\\n📈 Creating standardized visualizations...\")\n",
    "    main_figure = create_comprehensive_visualization(comparison_data, timeout_data, task_detection_rates)\n",
    "    \n",
    "    # Display summary table\n",
    "    print(\"\\n📋 Creating summary table...\")\n",
    "    summary_html = create_summary_table(comparison_data)\n",
    "    display(HTML(summary_html))\n",
    "    \n",
    "    print(\"\\n✅ Standardized analysis complete! All scrapers now use consistent evaluation:\")\n",
    "    print(\"   🎯 Domain-based aggregation with standardized classification\")\n",
    "    print(\"   📊 Weighted scoring system: 1pt single indicator, 2pts both indicators\") \n",
    "    print(\"   📈 Consistent visualization across all notebooks\")\n",
    "    print(\"   🔽 Run the next cell to save all visualizations to PNG files.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error connecting to database or processing data: {e}\")\n",
    "    print(\"Please check if MongoDB is running and accessible.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075c6641",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Save Combined Visualization - Grafiken und HTML Export\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib\n",
    "matplotlib.rcParams['font.family'] = 'DejaVu Sans'  # Fix font warning\n",
    "\n",
    "# Erstelle Output-Ordner\n",
    "output_dir = r\"scraper_comparison_results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Timestamp für eindeutige Dateinamen\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "try:\n",
    "    # 1. Speichere die Hauptvisualisierung aus der ersten Zelle\n",
    "    if 'main_figure' in locals() and main_figure:\n",
    "        print(\"💾 Saving combined visualization...\")\n",
    "        \n",
    "        # PNG für hohe Qualität\n",
    "        png_path = os.path.join(output_dir, f\"scraper_comparison_complete_{timestamp}.png\")\n",
    "        main_figure.savefig(png_path, dpi=300, bbox_inches='tight', facecolor='white', edgecolor='none')\n",
    "        \n",
    "        # PDF für Vektorgrafik\n",
    "        pdf_path = os.path.join(output_dir, f\"scraper_comparison_complete_{timestamp}.pdf\")\n",
    "        main_figure.savefig(pdf_path, bbox_inches='tight', facecolor='white', edgecolor='none')\n",
    "        \n",
    "        print(f\"   ✅ Hauptvisualisierung gespeichert: {os.path.basename(png_path)}\")\n",
    "    else:\n",
    "        print(\"⚠️ Keine Hauptvisualisierung zum Speichern gefunden. Führe zuerst die erste Zelle aus.\")\n",
    "\n",
    "    # 2. Speichere HTML-Zusammenfassung\n",
    "    if 'summary_html' in locals() and summary_html:\n",
    "        html_path = os.path.join(output_dir, f\"scraper_comparison_summary_{timestamp}.html\")\n",
    "        with open(html_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"\"\"<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>Scraper Comparison Results - {timestamp}</title>\n",
    "</head>\n",
    "<body>\n",
    "{summary_html}\n",
    "</body>\n",
    "</html>\"\"\")\n",
    "        print(f\"📋 HTML-Zusammenfassung gespeichert: {os.path.basename(html_path)}\")\n",
    "\n",
    "    # 3. Zusätzlich: JSON-Export der Vergleichsdaten\n",
    "    if 'comparison_data' in locals() and comparison_data:\n",
    "        import json\n",
    "        \n",
    "        json_path = os.path.join(output_dir, f\"scraper_comparison_data_{timestamp}.json\")\n",
    "        with open(json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(comparison_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"📊 Vergleichsdaten gespeichert: {os.path.basename(json_path)}\")\n",
    "\n",
    "    # 4. Task Detection Rates als CSV\n",
    "    if 'task_detection_rates' in locals() and task_detection_rates:\n",
    "        import pandas as pd\n",
    "        \n",
    "        csv_path = os.path.join(output_dir, f\"task_detection_rates_{timestamp}.csv\")\n",
    "        pd.DataFrame(task_detection_rates).to_csv(csv_path, index=False, encoding='utf-8')\n",
    "        print(f\"📈 Task Detection Rates gespeichert: {os.path.basename(csv_path)}\")\n",
    "\n",
    "    # 5. Timeout Data als CSV\n",
    "    if 'timeout_data' in locals() and timeout_data:\n",
    "        import pandas as pd\n",
    "        \n",
    "        timeout_csv_path = os.path.join(output_dir, f\"timeout_compliance_{timestamp}.csv\")\n",
    "        pd.DataFrame(timeout_data).to_csv(timeout_csv_path, index=False, encoding='utf-8')\n",
    "        print(f\"⏱️ Timeout Compliance gespeichert: {os.path.basename(timeout_csv_path)}\")\n",
    "\n",
    "    # 6. Statistik-Zusammenfassung als TXT\n",
    "    if 'comparison_data' in locals() and comparison_data:\n",
    "        stats_path = os.path.join(output_dir, f\"scraper_comparison_statistics_{timestamp}.txt\")\n",
    "        \n",
    "        with open(stats_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"Scraper Comparison Results Summary - {timestamp}\\n\")\n",
    "            f.write(\"=\"*60 + \"\\n\\n\")\n",
    "            \n",
    "            for scraper, data in comparison_data.items():\n",
    "                f.write(f\"{scraper.replace('_', ' ').title()}:\\n\")\n",
    "                f.write(f\"  Total Domains: {data['total_domains']}\\n\")\n",
    "                f.write(f\"  Total URLs: {data['total_urls']}\\n\")\n",
    "                f.write(f\"  Average Duration/URL: {data['avg_duration_per_url']:.1f}s\\n\")\n",
    "                f.write(f\"  Secure Indicators: {data['secure_passkey']}\\n\")\n",
    "                f.write(f\"  Possible Indicators: {data['possible_passkey']}\\n\")\n",
    "                f.write(f\"  FedCM Indicators: {data['fedcm']}\\n\")\n",
    "                success_rate = (data['secure_passkey'] / data['total_domains'] * 100) if data['total_domains'] > 0 else 0\n",
    "                f.write(f\"  Success Rate: {success_rate:.1f}%\\n\\n\")\n",
    "        \n",
    "        print(f\"📊 Statistik-Zusammenfassung gespeichert: {os.path.basename(stats_path)}\")\n",
    "\n",
    "    print(f\"\\n✅ Alle Dateien gespeichert in: {output_dir}\")\n",
    "    print(\"📁 Gespeicherte Dateien:\")\n",
    "    for file in sorted(os.listdir(output_dir)):\n",
    "        if timestamp in file:\n",
    "            file_path = os.path.join(output_dir, file)\n",
    "            file_size = os.path.getsize(file_path) / 1024  # KB\n",
    "            print(f\"   📄 {file} ({file_size:.1f} KB)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Fehler beim Speichern: {e}\")\n",
    "    print(\"💡 Stelle sicher, dass die erste Zelle erfolgreich ausgeführt wurde.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

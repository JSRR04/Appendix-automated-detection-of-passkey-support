{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14c226c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Network Scraper Results Analysis with Standardized Domain-Based Processing\n",
    "# Unified evaluation system consistent with all scrapers\n",
    "# Two-cell structure: Analysis + Export\n",
    "\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from urllib.parse import urlparse\n",
    "from IPython.display import HTML, display\n",
    "import json\n",
    "import html\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "MONGO_HOST = 'mongo'\n",
    "MONGO_PORT = 27017\n",
    "MONGO_USER = 'admin'\n",
    "MONGO_PASS = 'changeme'\n",
    "MONGO_DB_NAME = 'tasks'\n",
    "\n",
    "# Support for multiple task names - empty list means analyze ALL tasks\n",
    "TASK_NAMES_TO_ANALYZE = []\n",
    "\n",
    "def get_main_domain_from_url(url):\n",
    "    \"\"\"Extract main domain from URL (consistent with scrapers)\"\"\"\n",
    "    try:\n",
    "        parsed_url = urlparse(url)\n",
    "        netloc = parsed_url.netloc\n",
    "        \n",
    "        subdomains_to_remove = [\"sso.\", \"idp.\", \"login.\", \"www.\"]\n",
    "        for subdomain in subdomains_to_remove:\n",
    "            if netloc.startswith(subdomain):\n",
    "                netloc = netloc[len(subdomain):]\n",
    "        \n",
    "        parts = netloc.split(\".\")\n",
    "        if len(parts) > 2:\n",
    "            netloc = \".\".join(parts[-2:])\n",
    "        \n",
    "        return f\"{parsed_url.scheme}://{netloc}\"\n",
    "    except Exception:\n",
    "        return url\n",
    "\n",
    "def detect_secure_html_indicators(url_data):\n",
    "    \"\"\"\n",
    "    STANDARDIZED: Detect secure HTML indicators (consistent across all scrapers)\n",
    "    Returns True if domain has webauthn inputs or passkey buttons\n",
    "    \"\"\"\n",
    "    return (url_data.get('webauthn_input_found', False) or \n",
    "            url_data.get('passkey_button_found', False))\n",
    "\n",
    "def detect_secure_specific_indicators(url_data):\n",
    "    \"\"\"\n",
    "    STANDARDIZED: Detect secure specific indicators for network_scraper\n",
    "    Returns True if domain has secure network requests or responses\n",
    "    \"\"\"\n",
    "    secure_requests = url_data.get('secure_passkey_requests', [])\n",
    "    secure_responses = url_data.get('secure_passkey_responses', [])\n",
    "    return ((isinstance(secure_requests, list) and len(secure_requests) > 0) or\n",
    "            (isinstance(secure_responses, list) and len(secure_responses) > 0))\n",
    "\n",
    "def detect_possible_indicators(url_data):\n",
    "    \"\"\"\n",
    "    STANDARDIZED: Detect possible indicators for network_scraper\n",
    "    Returns True if domain has possible network patterns or requests\n",
    "    \"\"\"\n",
    "    possible_req = url_data.get('possible_passkey_requests', [])\n",
    "    possible_resp = url_data.get('possible_passkey_responses', [])\n",
    "    passkey_req = url_data.get('passkey_requests', [])\n",
    "    passkey_resp = url_data.get('passkey_responses', [])\n",
    "    patterns = url_data.get('passkey_patterns_detected', False)\n",
    "    \n",
    "    return ((isinstance(possible_req, list) and len(possible_req) > 0) or\n",
    "            (isinstance(possible_resp, list) and len(possible_resp) > 0) or\n",
    "            (isinstance(passkey_req, list) and len(passkey_req) > 0) or\n",
    "            (isinstance(passkey_resp, list) and len(passkey_resp) > 0) or\n",
    "            patterns)\n",
    "\n",
    "def detect_fedcm_indicators(url_data):\n",
    "    \"\"\"\n",
    "    STANDARDIZED: Detect FedCM indicators for network_scraper\n",
    "    Returns True if domain has FedCM network detections\n",
    "    \"\"\"\n",
    "    fedcm_detections = url_data.get('fedcm_detections', [])\n",
    "    return (isinstance(fedcm_detections, list) and len(fedcm_detections) > 0)\n",
    "\n",
    "def evaluate_domain_classification(domain_data):\n",
    "    \"\"\"\n",
    "    STANDARDIZED domain classification logic used across all scrapers.\n",
    "    Each domain is classified into exactly ONE category with weighted scoring:\n",
    "    - Secure Both (HTML + Specific): 2 points\n",
    "    - Secure HTML Only: 1 point\n",
    "    - Secure Specific Only: 1 point\n",
    "    - No Secure Indicators: 0 points\n",
    "    \"\"\"\n",
    "    # Check all URLs in domain for any indicators\n",
    "    has_secure_specific = False\n",
    "    has_secure_html = False\n",
    "    has_possible = False\n",
    "    has_fedcm = False\n",
    "    \n",
    "    for url_data in domain_data:\n",
    "        if detect_secure_specific_indicators(url_data):\n",
    "            has_secure_specific = True\n",
    "        if detect_secure_html_indicators(url_data):\n",
    "            has_secure_html = True\n",
    "        if detect_possible_indicators(url_data):\n",
    "            has_possible = True\n",
    "        if detect_fedcm_indicators(url_data):\n",
    "            has_fedcm = True\n",
    "    \n",
    "    # Classify domain based on secure indicators\n",
    "    classification = {\n",
    "        'secure_html_only': False,\n",
    "        'secure_specific_only': False,\n",
    "        'secure_both': False,\n",
    "        'no_secure_found': False,\n",
    "        'has_possible': has_possible,\n",
    "        'has_fedcm': has_fedcm,\n",
    "        'weighted_score': 0\n",
    "    }\n",
    "    \n",
    "    if has_secure_html and has_secure_specific:\n",
    "        classification['secure_both'] = True\n",
    "        classification['weighted_score'] = 2\n",
    "    elif has_secure_html:\n",
    "        classification['secure_html_only'] = True\n",
    "        classification['weighted_score'] = 1\n",
    "    elif has_secure_specific:\n",
    "        classification['secure_specific_only'] = True\n",
    "        classification['weighted_score'] = 1\n",
    "    else:\n",
    "        classification['no_secure_found'] = True\n",
    "        classification['weighted_score'] = 0\n",
    "    \n",
    "    return classification\n",
    "\n",
    "def calculate_indicators_per_task(df, domain_groups):\n",
    "    \"\"\"Calculate indicators per task_name for task-specific visualization\"\"\"\n",
    "    task_indicators = {}\n",
    "    \n",
    "    if 'task_name' in df.columns:\n",
    "        for task_name in df['task_name'].unique():\n",
    "            # Get domains that belong to this task\n",
    "            task_df = df[df['task_name'] == task_name]\n",
    "            task_domain_groups = defaultdict(list)\n",
    "            \n",
    "            for _, row in task_df.iterrows():\n",
    "                domain = row.get('url_id', 'unknown')\n",
    "                task_domain_groups[domain].append(row)\n",
    "            \n",
    "            # Calculate indicators for this task's domains\n",
    "            secure_html_only = 0\n",
    "            secure_specific_only = 0\n",
    "            secure_both = 0\n",
    "            possible_count = 0\n",
    "            fedcm_count = 0\n",
    "            \n",
    "            for domain, domain_data in task_domain_groups.items():\n",
    "                classification = evaluate_domain_classification(domain_data)\n",
    "                \n",
    "                if classification['secure_html_only']:\n",
    "                    secure_html_only += 1\n",
    "                elif classification['secure_specific_only']:\n",
    "                    secure_specific_only += 1\n",
    "                elif classification['secure_both']:\n",
    "                    secure_both += 1\n",
    "                    \n",
    "                if classification['has_possible']:\n",
    "                    possible_count += 1\n",
    "                if classification['has_fedcm']:\n",
    "                    fedcm_count += 1\n",
    "            \n",
    "            task_indicators[task_name] = {\n",
    "                'secure_html_only': secure_html_only,\n",
    "                'secure_specific_only': secure_specific_only,\n",
    "                'secure_both': secure_both,\n",
    "                'possible_passkey': possible_count,\n",
    "                'fedcm': fedcm_count,\n",
    "                'total_domains': len(task_domain_groups)\n",
    "            }\n",
    "    \n",
    "    return task_indicators\n",
    "\n",
    "def load_and_process_data(db, collection_name, task_names=None):\n",
    "    \"\"\"Load data from MongoDB and process it for domain-based analysis\"\"\"\n",
    "    collection = db[collection_name]\n",
    "    \n",
    "    # Build query for specific task names if provided - USE task_name at document level\n",
    "    query = {}\n",
    "    if task_names:\n",
    "        query = {\"task_name\": {\"$in\": task_names}}\n",
    "    \n",
    "    # Load all data for processing\n",
    "    data = list(collection.find(query))\n",
    "    if not data:\n",
    "        print(f\"No documents found in '{collection_name}' for tasks: {task_names}\")\n",
    "        return pd.DataFrame(), {}\n",
    "\n",
    "    normalized_data = []\n",
    "    for doc in data:\n",
    "        if 'result' in doc and doc['result'] is not None:\n",
    "            if isinstance(doc['result'], dict):\n",
    "                flat_doc = pd.json_normalize(doc['result']).to_dict(orient='records')[0]\n",
    "                \n",
    "                # FIXED: Get task_name from document level, not from result\n",
    "                if 'task_name' in doc:\n",
    "                    flat_doc['task_name'] = doc['task_name']\n",
    "                elif 'task_name' not in flat_doc:\n",
    "                    flat_doc['task_name'] = 'unknown'\n",
    "                \n",
    "                # Handle timeout and duration\n",
    "                if 'task_config.timeout' in flat_doc:\n",
    "                    flat_doc['timeout'] = flat_doc['task_config.timeout']\n",
    "                elif 'timeout' not in flat_doc:\n",
    "                    flat_doc['timeout'] = 60\n",
    "                    \n",
    "                if 'duration_seconds' not in flat_doc:\n",
    "                    flat_doc['duration_seconds'] = np.nan\n",
    "                \n",
    "                # Ensure url_id from result\n",
    "                if 'url_id' not in flat_doc and 'url' in flat_doc:\n",
    "                    try:\n",
    "                        flat_doc['url_id'] = get_main_domain_from_url(flat_doc['url'])\n",
    "                    except:\n",
    "                        flat_doc['url_id'] = 'unknown'\n",
    "                    \n",
    "                # Add default fields if missing\n",
    "                for field in ['secure_passkey_requests', 'secure_passkey_responses', \n",
    "                             'possible_passkey_requests', 'possible_passkey_responses',\n",
    "                             'passkey_requests', 'passkey_responses', 'fedcm_detections']:\n",
    "                    if field not in flat_doc:\n",
    "                        flat_doc[field] = []\n",
    "                        \n",
    "                if 'passkey_patterns_detected' not in flat_doc:\n",
    "                    flat_doc['passkey_patterns_detected'] = False\n",
    "                    \n",
    "                # Handle NaN values in boolean columns\n",
    "                for bool_col in ['passkey_patterns_detected', 'error', 'passkey_button_found', 'webauthn_input_found']:\n",
    "                    if bool_col in flat_doc and pd.isna(flat_doc[bool_col]):\n",
    "                        flat_doc[bool_col] = False\n",
    "                \n",
    "                if 'error_messages' not in flat_doc:\n",
    "                    flat_doc['error_messages'] = []\n",
    "                        \n",
    "                normalized_data.append(flat_doc)\n",
    "    \n",
    "    df = pd.DataFrame(normalized_data)\n",
    "    print(f\"{len(df)} documents loaded from '{collection_name}' for tasks: {task_names}\")\n",
    "    \n",
    "    if not df.empty and 'task_name' in df.columns:\n",
    "        found_tasks = df['task_name'].unique()\n",
    "        print(f\"Tasks found in data: {', '.join(found_tasks)}\")\n",
    "    \n",
    "    # Group by domain (url_id) for domain-based processing\n",
    "    domain_groups = defaultdict(list)\n",
    "    for _, row in df.iterrows():\n",
    "        domain = row.get('url_id', 'unknown')\n",
    "        domain_groups[domain].append(row)\n",
    "    \n",
    "    return df, domain_groups\n",
    "\n",
    "def calculate_standardized_metrics(df, domain_groups):\n",
    "    \"\"\"Calculate metrics using standardized domain classification\"\"\"\n",
    "    total_domains = len(domain_groups)\n",
    "    \n",
    "    # Counters using standardized classification\n",
    "    secure_html_only = 0\n",
    "    secure_specific_only = 0\n",
    "    secure_both = 0\n",
    "    possible_count = 0\n",
    "    fedcm_count = 0\n",
    "    weighted_total = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    for domain, domain_data in domain_groups.items():\n",
    "        classification = evaluate_domain_classification(domain_data)\n",
    "        \n",
    "        if classification['secure_html_only']:\n",
    "            secure_html_only += 1\n",
    "        elif classification['secure_specific_only']:\n",
    "            secure_specific_only += 1\n",
    "        elif classification['secure_both']:\n",
    "            secure_both += 1\n",
    "            \n",
    "        if classification['has_possible']:\n",
    "            possible_count += 1\n",
    "        if classification['has_fedcm']:\n",
    "            fedcm_count += 1\n",
    "            \n",
    "        weighted_total += classification['weighted_score']\n",
    "        \n",
    "        # Check for errors in any URL of this domain\n",
    "        for url_data in domain_data:\n",
    "            if url_data.get('error', False):\n",
    "                error_count += 1\n",
    "                break\n",
    "    \n",
    "    total_secure = secure_html_only + secure_specific_only + secure_both\n",
    "    \n",
    "    # Timeout analysis for all URLs\n",
    "    timeout_analysis = analyze_timeouts(df)\n",
    "    \n",
    "    # Calculate per-task indicators\n",
    "    task_indicators = calculate_indicators_per_task(df, domain_groups)\n",
    "    \n",
    "    metrics = {\n",
    "        'total_domains': total_domains,\n",
    "        'total_urls': len(df),\n",
    "        'secure_html_only': secure_html_only,\n",
    "        'secure_specific_only': secure_specific_only,\n",
    "        'secure_both': secure_both,\n",
    "        'total_secure': total_secure,\n",
    "        'weighted_total': weighted_total,\n",
    "        'possible_passkey': possible_count,\n",
    "        'fedcm': fedcm_count,\n",
    "        'errors': error_count,\n",
    "        'task_indicators': task_indicators\n",
    "    }\n",
    "    \n",
    "    return metrics, timeout_analysis\n",
    "\n",
    "def analyze_timeouts(df):\n",
    "    \"\"\"Analyze timeout compliance for all URLs per task\"\"\"\n",
    "    timeout_data = []\n",
    "    \n",
    "    if 'task_name' in df.columns:\n",
    "        for task_name in df['task_name'].unique():\n",
    "            task_df = df[df['task_name'] == task_name]\n",
    "            if not task_df.empty and 'timeout' in task_df.columns and 'duration_seconds' in task_df.columns:\n",
    "                timeout_val = task_df['timeout'].iloc[0]\n",
    "                if pd.notna(timeout_val) and timeout_val > 0:\n",
    "                    valid_durations = task_df.dropna(subset=['duration_seconds'])\n",
    "                    if not valid_durations.empty:\n",
    "                        compliance_ratios = valid_durations['duration_seconds'] / timeout_val\n",
    "                        \n",
    "                        counts = {\n",
    "                            '0-50%': int((compliance_ratios <= 0.5).sum()),\n",
    "                            '50-100%': int(((compliance_ratios > 0.5) & (compliance_ratios <= 1.0)).sum()),\n",
    "                            '100-150%': int(((compliance_ratios > 1.0) & (compliance_ratios <= 1.5)).sum()),\n",
    "                            '>150%': int((compliance_ratios > 1.5).sum())\n",
    "                        }\n",
    "                        \n",
    "                        timeout_data.append({\n",
    "                            'task_name': task_name,\n",
    "                            'timeout': int(timeout_val),\n",
    "                            'total_urls': len(valid_durations),\n",
    "                            **counts\n",
    "                        })\n",
    "    \n",
    "    return timeout_data\n",
    "\n",
    "def create_standardized_visualization(metrics, timeout_analysis, domain_groups):\n",
    "    \"\"\"Create standardized visualization with 6 plots - task indicators spanning full width at the end\"\"\"\n",
    "    fig = plt.figure(figsize=(16, 22))\n",
    "    fig.patch.set_facecolor('#fafafa')\n",
    "    fig.suptitle('Network Scraper - Standardized Domain-Based Analysis', \n",
    "                fontsize=18, fontweight='bold', color='#2d1b69', y=0.99)\n",
    "    \n",
    "    # Create subplot layout using subplot2grid (compatible with tight_layout)\n",
    "    ax1 = plt.subplot2grid((4, 2), (0, 0))\n",
    "    ax2 = plt.subplot2grid((4, 2), (0, 1))\n",
    "    ax3 = plt.subplot2grid((4, 2), (1, 0))\n",
    "    ax4 = plt.subplot2grid((4, 2), (1, 1))\n",
    "    ax5 = plt.subplot2grid((4, 2), (2, 0), colspan=2)\n",
    "    ax6 = plt.subplot2grid((4, 2), (3, 0), colspan=2)\n",
    "    \n",
    "    # Modern colors matching comparison\n",
    "    colors = {\n",
    "        'primary': '#3498db',      # HTML only\n",
    "        'secondary': '#2ecc71',    # Specific only  \n",
    "        'both': '#9b59b6',        # Both indicators\n",
    "        'possible': '#f39c12',     # Possible indicators\n",
    "        'fedcm': '#3742fa',        # FedCM indicators\n",
    "        'success': '#20bf6b',      # Success metrics\n",
    "        'warning': '#ff6b6b'       # Warnings/errors\n",
    "    }\n",
    "    \n",
    "    # 1. Domain vs URL Count\n",
    "    counts = [metrics['total_domains'], metrics['total_urls']]\n",
    "    bars1 = ax1.bar(['Unique Domains', 'Total URLs'], counts, \n",
    "                   color=[colors['primary'], colors['secondary']], alpha=0.8, \n",
    "                   edgecolor='white', linewidth=2)\n",
    "    ax1.set_title('Domains vs URLs', fontsize=14, fontweight='bold', color='#2d1b69',pad=20)\n",
    "    ax1.set_ylabel('Count', fontsize=12, color='#374151')\n",
    "    \n",
    "    for bar, count in zip(bars1, counts):\n",
    "        percentage = (count / sum(counts)) * 100\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(counts)*0.01, \n",
    "                f'{count} ({percentage:.1f}%)', ha='center', va='bottom', \n",
    "                fontweight='bold', fontsize=11, color='#2d1b69')\n",
    "    \n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 2. Standardized Indicator Classification with Stacked Bars\n",
    "    categories = ['Secure Indicators', 'Possible', 'FedCM']\n",
    "    \n",
    "    # Create stacked bar for secure indicators\n",
    "    secure_html_data = [metrics['secure_html_only'], 0, 0]\n",
    "    secure_specific_data = [metrics['secure_specific_only'], 0, 0] \n",
    "    secure_both_data = [metrics['secure_both'], 0, 0]\n",
    "    possible_data = [0, metrics['possible_passkey'], 0]\n",
    "    fedcm_data = [0, 0, metrics['fedcm']]\n",
    "    \n",
    "    # Stacked bars for secure indicators\n",
    "    bars2_html = ax2.bar(categories, secure_html_data,\n",
    "                        color=colors['primary'], alpha=0.8, edgecolor='white', linewidth=1,\n",
    "                        label='HTML Only (1pt)')\n",
    "    bars2_specific = ax2.bar(categories, secure_specific_data, bottom=secure_html_data,\n",
    "                           color=colors['secondary'], alpha=0.8, edgecolor='white', linewidth=1,\n",
    "                           label='Specific Only (1pt)')\n",
    "    bars2_both = ax2.bar(categories, secure_both_data,\n",
    "                        bottom=[h + s for h, s in zip(secure_html_data, secure_specific_data)],\n",
    "                        color=colors['both'], alpha=0.8, edgecolor='white', linewidth=1,\n",
    "                        label='Both (2pts)')\n",
    "    \n",
    "    # Separate bars for possible and fedcm\n",
    "    bars2_possible = ax2.bar(categories, possible_data,\n",
    "                           color=colors['possible'], alpha=0.8, edgecolor='white', linewidth=1,\n",
    "                           label='Possible')\n",
    "    bars2_fedcm = ax2.bar(categories, fedcm_data,\n",
    "                         color=colors['fedcm'], alpha=0.8, edgecolor='white', linewidth=1,\n",
    "                         label='FedCM')\n",
    "    \n",
    "    # Add numbers INSIDE the bar segments for secure indicators (first category only)\n",
    "    if secure_html_data[0] > 0:  # HTML only count\n",
    "        ax2.text(0, secure_html_data[0]/2, str(secure_html_data[0]), \n",
    "                ha='center', va='center', fontweight='bold', fontsize=11, color='white')\n",
    "    \n",
    "    if secure_specific_data[0] > 0:  # Specific only count\n",
    "        y_pos = secure_html_data[0] + secure_specific_data[0]/2\n",
    "        ax2.text(0, y_pos, str(secure_specific_data[0]), \n",
    "                ha='center', va='center', fontweight='bold', fontsize=11, color='white')\n",
    "    \n",
    "    if secure_both_data[0] > 0:  # Both indicators count\n",
    "        y_pos = secure_html_data[0] + secure_specific_data[0] + secure_both_data[0]/2\n",
    "        ax2.text(0, y_pos, str(secure_both_data[0]), \n",
    "                ha='center', va='center', fontweight='bold', fontsize=11, color='white')\n",
    "    \n",
    "    # Add numbers inside other bars if they have values\n",
    "    if possible_data[1] > 0:  # Possible indicators\n",
    "        ax2.text(1, possible_data[1]/2, str(possible_data[1]), \n",
    "                ha='center', va='center', fontweight='bold', fontsize=11, color='white')\n",
    "    \n",
    "    if fedcm_data[2] > 0:  # FedCM indicators\n",
    "        ax2.text(2, fedcm_data[2]/2, str(fedcm_data[2]), \n",
    "                ha='center', va='center', fontweight='bold', fontsize=11, color='white')\n",
    "    \n",
    "    ax2.set_title('Standardized Indicator Classification (Domain-Based)', fontsize=14, fontweight='bold', color='#2d1b69',pad=20)\n",
    "    ax2.set_ylabel('Number of Domains', fontsize=12, color='#374151')\n",
    "    \n",
    "    # Add value labels\n",
    "    total_secure = metrics['secure_html_only'] + metrics['secure_specific_only'] + metrics['secure_both']\n",
    "    bar_totals = [total_secure, metrics['possible_passkey'], metrics['fedcm']]\n",
    "    \n",
    "    for i, (category, total) in enumerate(zip(categories, bar_totals)):\n",
    "        if total > 0:\n",
    "            percentage = (total / metrics['total_domains']) * 100 if metrics['total_domains'] > 0 else 0\n",
    "            label_text = f'{total} ({percentage:.1f}%)'\n",
    "            if i == 0:  # Secure indicators - add weighted score\n",
    "                label_text += f'\\n{metrics[\"weighted_total\"]} pts'\n",
    "            ax2.text(i, total + max(bar_totals)*0.01, label_text, \n",
    "                    ha='center', va='bottom', fontweight='bold', fontsize=10, color='#2d1b69')\n",
    "    \n",
    "    ax2.legend(loc='upper right', frameon=True, fontsize=9)\n",
    "    ax2.spines['top'].set_visible(False) \n",
    "    ax2.spines['right'].set_visible(False)\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 3. Detection Overview Pie Chart with Weighted Scores\n",
    "    if metrics['total_secure'] > 0 or metrics['possible_passkey'] > 0:\n",
    "        pie_labels = []\n",
    "        pie_values = []\n",
    "        pie_colors = []\n",
    "        \n",
    "        if metrics['secure_html_only'] > 0:\n",
    "            pie_labels.append(f'HTML Only\\n({metrics[\"secure_html_only\"]} domains, 1pt each)')\n",
    "            pie_values.append(metrics['secure_html_only'])\n",
    "            pie_colors.append(colors['primary'])\n",
    "            \n",
    "        if metrics['secure_specific_only'] > 0:\n",
    "            pie_labels.append(f'Specific Only\\n({metrics[\"secure_specific_only\"]} domains, 1pt each)')\n",
    "            pie_values.append(metrics['secure_specific_only'])\n",
    "            pie_colors.append(colors['secondary'])\n",
    "            \n",
    "        if metrics['secure_both'] > 0:\n",
    "            pie_labels.append(f'Both Indicators\\n({metrics[\"secure_both\"]} domains, 2pts each)')\n",
    "            pie_values.append(metrics['secure_both'])\n",
    "            pie_colors.append(colors['both'])\n",
    "            \n",
    "        if metrics['possible_passkey'] > 0:\n",
    "            pie_labels.append(f'Possible Only\\n({metrics[\"possible_passkey\"]} domains)')\n",
    "            pie_values.append(metrics['possible_passkey'])\n",
    "            pie_colors.append(colors['possible'])\n",
    "            \n",
    "        no_indicators = metrics['total_domains'] - total_secure - metrics['possible_passkey']\n",
    "        if no_indicators > 0:\n",
    "            pie_labels.append(f'No Indicators\\n({no_indicators} domains)')\n",
    "            pie_values.append(no_indicators)\n",
    "            pie_colors.append('#6c757d')\n",
    "        \n",
    "        wedges, texts, autotexts = ax3.pie(pie_values, labels=pie_labels,\n",
    "                                          autopct='%1.1f%%', startangle=90,\n",
    "                                          colors=pie_colors, textprops={'fontsize': 9},\n",
    "                                          wedgeprops={'edgecolor': 'white', 'linewidth': 2})\n",
    "        \n",
    "        ax3.set_title(f'Domain Classification Overview\\nWeighted Score: {metrics[\"weighted_total\"]} points', \n",
    "                     fontsize=14, fontweight='bold', color='#2d1b69',pad=20)\n",
    "    else:\n",
    "        ax3.text(0.5, 0.5, 'No indicators found', ha='center', va='center',\n",
    "                transform=ax3.transAxes, fontsize=14, color='#6b7280')\n",
    "        ax3.set_title('Domain Classification Overview', fontsize=14, fontweight='bold', color='#2d1b69',pad=20)\n",
    "    \n",
    "    # 4. Timeout Compliance\n",
    "    if timeout_analysis:\n",
    "        timeout_df = pd.DataFrame(timeout_analysis)\n",
    "        tasks = timeout_df['task_name'].tolist()\n",
    "        \n",
    "        categories = ['0-50%', '50-100%', '100-150%', '>150%']\n",
    "        colors_timeout = ['#90EE90', '#28a745', '#ffc107', '#dc3545']\n",
    "        \n",
    "        bottom = np.zeros(len(tasks))\n",
    "        for i, category in enumerate(categories):\n",
    "            values = timeout_df[category].tolist()\n",
    "            bars = ax4.bar(tasks, values, bottom=bottom, label=category,\n",
    "                          color=colors_timeout[i], alpha=0.8, edgecolor='white', linewidth=1)\n",
    "            \n",
    "            # Add absolute numbers on bars\n",
    "            for j, (bar, value) in enumerate(zip(bars, values)):\n",
    "                if value > 0:\n",
    "                    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_y() + bar.get_height()/2,\n",
    "                            str(value), ha='center', va='center', fontweight='bold',\n",
    "                            fontsize=9, color='white' if i in [1, 3] else 'black')\n",
    "            \n",
    "            bottom += values\n",
    "        \n",
    "        ax4.set_title('Timeout Compliance (All URLs per Task)', fontsize=14, fontweight='bold', color='#2d1b69',pad=20)\n",
    "        ax4.set_xlabel('Task', fontsize=12, color='#374151')\n",
    "        ax4.set_ylabel('Number of URLs', fontsize=12, color='#374151')\n",
    "        ax4.legend(frameon=True, fontsize=10)\n",
    "        ax4.tick_params(axis='x', rotation=45, colors='#374151')\n",
    "        ax4.spines['top'].set_visible(False)\n",
    "        ax4.spines['right'].set_visible(False)\n",
    "        ax4.grid(axis='y', alpha=0.3)\n",
    "    else:\n",
    "        ax4.text(0.5, 0.5, 'No timeout data available', ha='center', va='center',\n",
    "                transform=ax4.transAxes, fontsize=14, color='#6b7280')\n",
    "        ax4.set_title('Timeout Analysis', fontsize=14, fontweight='bold', color='#2d1b69',pad=20)\n",
    "    \n",
    "    # 5. Domain Classification by Indicators and Errors (spanning full width)\n",
    "    indicator_error_data = classify_domains_by_indicators_and_errors(domain_groups)\n",
    "    \n",
    "    categories = ['Secure + No Error', 'Secure + Error', 'No Secure + No Error', 'No Secure + Error']\n",
    "    counts = [\n",
    "        indicator_error_data['secure_no_error'],\n",
    "        indicator_error_data['secure_with_error'], \n",
    "        indicator_error_data['no_secure_no_error'],\n",
    "        indicator_error_data['no_secure_with_error']\n",
    "    ]\n",
    "    \n",
    "    colors_bars = ['#20bf6b', '#f39c12', '#6c757d', '#ff6b6b']\n",
    "    bars5 = ax5.bar(categories, counts, color=colors_bars, alpha=0.8, edgecolor='white', linewidth=2)\n",
    "    \n",
    "    ax5.set_title('Domains by Indicators and Error Status', fontsize=14, fontweight='bold', color='#2d1b69',pad=20)\n",
    "    ax5.set_ylabel('Number of Domains', fontsize=12, color='#374151')\n",
    "    ax5.set_xlabel('Domain Category', fontsize=12, color='#374151')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    max_count = max(counts) if counts else 1\n",
    "    for bar, count in zip(bars5, counts):\n",
    "        percentage = (count / sum(counts)) * 100 if sum(counts) > 0 else 0\n",
    "        ax5.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max_count*0.01,\n",
    "                f'{count}\\n({percentage:.1f}%)', ha='center', va='bottom', \n",
    "                fontweight='bold', fontsize=10, color='#2d1b69')\n",
    "    \n",
    "    ax5.tick_params(axis='x', rotation=45, colors='#374151')\n",
    "    ax5.spines['top'].set_visible(False)\n",
    "    ax5.spines['right'].set_visible(False)\n",
    "    ax5.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 6. Task-specific Indicators (spanning full width)\n",
    "    task_indicators = metrics.get('task_indicators', {})\n",
    "    if task_indicators:\n",
    "        task_names = list(task_indicators.keys())\n",
    "        \n",
    "        # Set up bar positions\n",
    "        x = np.arange(len(task_names))\n",
    "        width = 0.25\n",
    "        \n",
    "        # Data for each indicator type\n",
    "        secure_html_data = [task_indicators[task]['secure_html_only'] for task in task_names]\n",
    "        secure_specific_data = [task_indicators[task]['secure_specific_only'] for task in task_names]\n",
    "        secure_both_data = [task_indicators[task]['secure_both'] for task in task_names]\n",
    "        possible_data = [task_indicators[task]['possible_passkey'] for task in task_names]\n",
    "        fedcm_data = [task_indicators[task]['fedcm'] for task in task_names]\n",
    "        \n",
    "        # Create grouped bars for each category\n",
    "        # Secure indicators (stacked)\n",
    "        bars_html = ax6.bar(x - width, secure_html_data, width, \n",
    "                          color=colors['primary'], alpha=0.8, edgecolor='white', linewidth=1,\n",
    "                          label='HTML Only (1pt)')\n",
    "        bars_specific = ax6.bar(x - width, secure_specific_data, width, bottom=secure_html_data,\n",
    "                              color=colors['secondary'], alpha=0.8, edgecolor='white', linewidth=1,\n",
    "                              label='Specific Only (1pt)')\n",
    "        bars_both = ax6.bar(x - width, secure_both_data, width, \n",
    "                          bottom=[h + s for h, s in zip(secure_html_data, secure_specific_data)],\n",
    "                          color=colors['both'], alpha=0.8, edgecolor='white', linewidth=1,\n",
    "                          label='Both (2pts)')\n",
    "        \n",
    "        # Possible indicators\n",
    "        bars_possible = ax6.bar(x, possible_data, width,\n",
    "                              color=colors['possible'], alpha=0.8, edgecolor='white', linewidth=1,\n",
    "                              label='Possible')\n",
    "        \n",
    "        # FedCM indicators\n",
    "        bars_fedcm = ax6.bar(x + width, fedcm_data, width,\n",
    "                           color=colors['fedcm'], alpha=0.8, edgecolor='white', linewidth=1,\n",
    "                           label='FedCM')\n",
    "        \n",
    "        # Add labels for secure stacked bars\n",
    "        for i, task in enumerate(task_names):\n",
    "            x_pos = i - width\n",
    "            total_secure = (secure_html_data[i] + secure_specific_data[i] + secure_both_data[i])\n",
    "            if total_secure > 0:\n",
    "                # Calculate weighted score for this task\n",
    "                weighted_score = (secure_html_data[i] + secure_specific_data[i] + secure_both_data[i] * 2)\n",
    "                ax6.text(x_pos, total_secure + 0.1, f'{total_secure}\\n({weighted_score}pts)', \n",
    "                       ha='center', va='bottom', fontweight='bold', fontsize=9, color='#2d1b69')\n",
    "        \n",
    "        # Add labels for possible and fedcm\n",
    "        def add_value_labels_ax6(bars, values):\n",
    "            for bar, value in zip(bars, values):\n",
    "                if value > 0:\n",
    "                    ax6.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.1,\n",
    "                           f'{value}', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "        \n",
    "        add_value_labels_ax6(bars_possible, possible_data)\n",
    "        add_value_labels_ax6(bars_fedcm, fedcm_data)\n",
    "        \n",
    "        # Customize the plot\n",
    "        ax6.set_title('Indicator Classification per Task Name (Domain-Based)', \n",
    "                    fontsize=14, fontweight='bold', color='#2d1b69', pad=20)\n",
    "        ax6.set_xlabel('Task Name', fontsize=12, color='#374151')\n",
    "        ax6.set_ylabel('Number of Domains', fontsize=12, color='#374151')\n",
    "        ax6.set_xticks(x)\n",
    "        ax6.set_xticklabels(task_names, rotation=45, ha='right', color='#374151')\n",
    "        \n",
    "        # Add legend\n",
    "        ax6.legend(loc='upper right', frameon=True, fontsize=10)\n",
    "        \n",
    "        # Style the plot\n",
    "        ax6.spines['top'].set_visible(False)\n",
    "        ax6.spines['right'].set_visible(False)\n",
    "        ax6.grid(axis='y', alpha=0.3)\n",
    "    else:\n",
    "        ax6.text(0.5, 0.5, 'No task indicators data available', ha='center', va='center',\n",
    "                transform=ax6.transAxes, fontsize=14, color='#6b7280')\n",
    "        ax6.set_title('Task-specific Indicator Analysis', fontsize=14, fontweight='bold', color='#2d1b69',pad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def classify_domains_by_indicators_and_errors(domain_groups):\n",
    "    \"\"\"Classify domains by secure indicators and error status\"\"\"\n",
    "    secure_no_error = 0\n",
    "    secure_with_error = 0\n",
    "    no_secure_no_error = 0\n",
    "    no_secure_with_error = 0\n",
    "    error_details = {}\n",
    "    \n",
    "    for domain, domain_data in domain_groups.items():\n",
    "        # Check if domain has secure indicators\n",
    "        classification = evaluate_domain_classification(domain_data)\n",
    "        has_secure = not classification['no_secure_found']\n",
    "        \n",
    "        # Check if domain has errors\n",
    "        has_error = False\n",
    "        for url_data in domain_data:\n",
    "            if url_data.get('error', False):\n",
    "                has_error = True\n",
    "                # Collect error types\n",
    "                error_msg = url_data.get('error_messages', [])\n",
    "                if isinstance(error_msg, list) and error_msg:\n",
    "                    for msg in error_msg:\n",
    "                        if isinstance(msg, str):\n",
    "                            error_type = msg.split(':')[0] if ':' in msg else 'Other'\n",
    "                            error_details[error_type] = error_details.get(error_type, 0) + 1\n",
    "                elif isinstance(error_msg, str) and error_msg:\n",
    "                    error_type = error_msg.split(':')[0] if ':' in error_msg else 'Other'\n",
    "                    error_details[error_type] = error_details.get(error_type, 0) + 1\n",
    "                else:\n",
    "                    error_details['Unknown'] = error_details.get('Unknown', 0) + 1\n",
    "                break\n",
    "        \n",
    "        # Classify domain\n",
    "        if has_secure and not has_error:\n",
    "            secure_no_error += 1\n",
    "        elif has_secure and has_error:\n",
    "            secure_with_error += 1\n",
    "        elif not has_secure and not has_error:\n",
    "            no_secure_no_error += 1\n",
    "        else:  # not has_secure and has_error\n",
    "            no_secure_with_error += 1\n",
    "    \n",
    "    return {\n",
    "        'secure_no_error': secure_no_error,\n",
    "        'secure_with_error': secure_with_error,\n",
    "        'no_secure_no_error': no_secure_no_error,\n",
    "        'no_secure_with_error': no_secure_with_error,\n",
    "        'error_details': error_details\n",
    "    }\n",
    "\n",
    "def create_summary_table(metrics):\n",
    "    \"\"\"Create summary table matching comparison format\"\"\"\n",
    "    table_html = '''<h3>📊 Network Scraper - Standardized Domain Results</h3>\n",
    "    <table style=\"border-collapse: collapse; width: 100%; margin: 20px 0;\">\n",
    "        <tr style=\"background-color: #f2f2f2;\">\n",
    "            <th style=\"border: 1px solid #ddd; padding: 12px; text-align: left;\">Metric</th>\n",
    "            <th style=\"border: 1px solid #ddd; padding: 12px; text-align: center;\">Count</th>\n",
    "            <th style=\"border: 1px solid #ddd; padding: 12px; text-align: center;\">Percentage</th>\n",
    "            <th style=\"border: 1px solid #ddd; padding: 12px; text-align: center;\">Weighted Points</th>\n",
    "        </tr>'''\n",
    "    \n",
    "    total_domains = metrics['total_domains']\n",
    "    \n",
    "    rows = [\n",
    "        ('Total Domains', metrics['total_domains'], 100.0, '-'),\n",
    "        ('Total URLs', metrics['total_urls'], '-', '-'),\n",
    "        ('Secure HTML Only', metrics['secure_html_only'], \n",
    "         (metrics['secure_html_only']/total_domains*100) if total_domains > 0 else 0, \n",
    "         f\"{metrics['secure_html_only']} × 1\"),\n",
    "        ('Secure Specific Only', metrics['secure_specific_only'], \n",
    "         (metrics['secure_specific_only']/total_domains*100) if total_domains > 0 else 0,\n",
    "         f\"{metrics['secure_specific_only']} × 1\"),\n",
    "        ('Secure Both', metrics['secure_both'], \n",
    "         (metrics['secure_both']/total_domains*100) if total_domains > 0 else 0,\n",
    "         f\"{metrics['secure_both']} × 2\"),\n",
    "        ('Total Secure', metrics['total_secure'], \n",
    "         (metrics['total_secure']/total_domains*100) if total_domains > 0 else 0,\n",
    "         f\"{metrics['weighted_total']} pts\"),\n",
    "        ('Possible Indicators', metrics['possible_passkey'], \n",
    "         (metrics['possible_passkey']/total_domains*100) if total_domains > 0 else 0, '-'),\n",
    "        ('FedCM Indicators', metrics['fedcm'], \n",
    "         (metrics['fedcm']/total_domains*100) if total_domains > 0 else 0, '-'),\n",
    "        ('Errors', metrics['errors'], \n",
    "         (metrics['errors']/total_domains*100) if total_domains > 0 else 0, '-')\n",
    "    ]\n",
    "    \n",
    "    for metric, count, percentage, points in rows:\n",
    "        perc_str = f\"{percentage:.1f}%\" if isinstance(percentage, (int, float)) and percentage != '-' else str(percentage)\n",
    "        table_html += f'''\n",
    "        <tr>\n",
    "            <td style=\"border: 1px solid #ddd; padding: 12px; font-weight: bold;\">{metric}</td>\n",
    "            <td style=\"border: 1px solid #ddd; padding: 12px; text-align: center;\">{count}</td>\n",
    "            <td style=\"border: 1px solid #ddd; padding: 12px; text-align: center;\">{perc_str}</td>\n",
    "            <td style=\"border: 1px solid #ddd; padding: 12px; text-align: center; color: purple; font-weight: bold;\">{points}</td>\n",
    "        </tr>'''\n",
    "    \n",
    "    table_html += '''</table>\n",
    "    <p><strong>Standardized Classification:</strong></p>\n",
    "    <ul>\n",
    "        <li><strong>Secure HTML:</strong> webauthn_input_found OR passkey_button_found</li>\n",
    "        <li><strong>Secure Specific:</strong> secure network requests/responses with passkey patterns</li>\n",
    "        <li><strong>Weighted Scoring:</strong> HTML only OR Specific only = 1 point, Both = 2 points</li>\n",
    "        <li><strong>Domain Priority:</strong> Both > HTML only > Specific only > No secure indicators</li>\n",
    "    </ul>'''\n",
    "    \n",
    "    return table_html\n",
    "\n",
    "# ====================== MAIN EXECUTION ======================\n",
    "\n",
    "try:\n",
    "    # Connect to MongoDB\n",
    "    print(\"🔄 Starting MongoDB data extraction...\")\n",
    "    client = pymongo.MongoClient(f'mongodb://{MONGO_USER}:{MONGO_PASS}@{MONGO_HOST}:{MONGO_PORT}/')\n",
    "    db = client[MONGO_DB_NAME]\n",
    "    \n",
    "    print(\"📡 Loading data from network_scraper collection...\")\n",
    "    print(\"🎯 Using STANDARDIZED evaluation logic:\")\n",
    "    print(\"   • Secure HTML: webauthn_input_found OR passkey_button_found\")\n",
    "    print(\"   • Secure Specific: secure network requests/responses with passkey patterns\")\n",
    "    print(\"   • Classification: HTML only (1pt) | Specific only (1pt) | Both (2pts) | None (0pts)\")\n",
    "    \n",
    "    df, domain_groups = load_and_process_data(db, 'network_scraper', TASK_NAMES_TO_ANALYZE)\n",
    "    \n",
    "    if not df.empty:\n",
    "        print(f\"\\n✅ Data extraction complete! Found {len(domain_groups)} unique domains.\")\n",
    "        print(f\"📊 Data Overview: {len(df)} URLs across {len(domain_groups)} domains\")\n",
    "        \n",
    "        print(\"🔄 Calculating standardized metrics...\")\n",
    "        metrics, timeout_analysis = calculate_standardized_metrics(df, domain_groups)\n",
    "        \n",
    "        print(f\"\\n🎯 Standardized Results:\")\n",
    "        print(f\"   • HTML only: {metrics['secure_html_only']} domains (1pt each)\")\n",
    "        print(f\"   • Specific only: {metrics['secure_specific_only']} domains (1pt each)\")\n",
    "        print(f\"   • Both indicators: {metrics['secure_both']} domains (2pts each)\")\n",
    "        print(f\"   • Total secure: {metrics['total_secure']} domains\")\n",
    "        print(f\"   • Weighted score: {metrics['weighted_total']} points\")\n",
    "        print(f\"   • Possible: {metrics['possible_passkey']}, FedCM: {metrics['fedcm']}\")\n",
    "        \n",
    "        print(\"📈 Creating standardized visualizations...\")\n",
    "        main_figure = create_standardized_visualization(metrics, timeout_analysis, domain_groups)\n",
    "        \n",
    "        print(\"📋 Creating summary table...\")\n",
    "        summary_html = create_summary_table(metrics)\n",
    "        display(HTML(summary_html))\n",
    "        \n",
    "        # Store variables for export\n",
    "        visualizations = [main_figure]\n",
    "        \n",
    "        print(\"\\n✅ Standardized analysis complete!\")\n",
    "        print(\"   🎯 Network scraper now uses consistent evaluation with other scrapers\")\n",
    "        print(\"   📊 Domain-based classification with weighted scoring\")\n",
    "        print(\"   📈 Unified visualization style with task-specific analysis\")\n",
    "        print(\"   🔽 Run the next cell to save results and visualizations\")\n",
    "    else:\n",
    "        print(\"❌ No data found in the network_scraper collection for the specified task names.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    print(\"Please check if MongoDB is running and accessible.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29b4e66",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Export Results and Visualizations\n",
    "# Standardized export consistent with all scrapers\n",
    "\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Export configuration\n",
    "EXPORT_DIR = os.path.expanduser('~/jupyter-exports')\n",
    "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "scraper_name = 'network_scraper'\n",
    "\n",
    "try:\n",
    "    # 1. Export raw data as CSV\n",
    "    if not df.empty:\n",
    "        csv_filename = f'{scraper_name}_standardized_data_{timestamp}.csv'\n",
    "        csv_path = os.path.join(EXPORT_DIR, csv_filename)\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"📄 Raw data exported: {csv_path}\")\n",
    "    \n",
    "    # 2. Export domain classification details\n",
    "    if domain_groups:\n",
    "        domain_results = []\n",
    "        for domain, domain_data in domain_groups.items():\n",
    "            classification = evaluate_domain_classification(domain_data)\n",
    "            \n",
    "            # Collect indicator details\n",
    "            indicators = {\n",
    "                'secure_html_found': False,\n",
    "                'secure_network_found': False,\n",
    "                'possible_found': False,\n",
    "                'fedcm_found': False,\n",
    "                'indicator_counts': {\n",
    "                    'webauthn_inputs': 0,\n",
    "                    'passkey_buttons': 0,\n",
    "                    'secure_requests': 0,\n",
    "                    'secure_responses': 0,\n",
    "                    'possible_requests': 0,\n",
    "                    'possible_responses': 0,\n",
    "                    'passkey_requests': 0,\n",
    "                    'passkey_responses': 0,\n",
    "                    'fedcm_detections': 0,\n",
    "                    'pattern_matches': 0\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            for url_data in domain_data:\n",
    "                if detect_secure_html_indicators(url_data):\n",
    "                    indicators['secure_html_found'] = True\n",
    "                    if url_data.get('webauthn_input_found', False):\n",
    "                        indicators['indicator_counts']['webauthn_inputs'] += 1\n",
    "                    if url_data.get('passkey_button_found', False):\n",
    "                        indicators['indicator_counts']['passkey_buttons'] += 1\n",
    "                \n",
    "                if detect_secure_specific_indicators(url_data):\n",
    "                    indicators['secure_network_found'] = True\n",
    "                    secure_req = url_data.get('secure_passkey_requests', [])\n",
    "                    secure_resp = url_data.get('secure_passkey_responses', [])\n",
    "                    if isinstance(secure_req, list):\n",
    "                        indicators['indicator_counts']['secure_requests'] += len(secure_req)\n",
    "                    if isinstance(secure_resp, list):\n",
    "                        indicators['indicator_counts']['secure_responses'] += len(secure_resp)\n",
    "                \n",
    "                if detect_possible_indicators(url_data):\n",
    "                    indicators['possible_found'] = True\n",
    "                    poss_req = url_data.get('possible_passkey_requests', [])\n",
    "                    poss_resp = url_data.get('possible_passkey_responses', [])\n",
    "                    passkey_req = url_data.get('passkey_requests', [])\n",
    "                    passkey_resp = url_data.get('passkey_responses', [])\n",
    "                    if isinstance(poss_req, list):\n",
    "                        indicators['indicator_counts']['possible_requests'] += len(poss_req)\n",
    "                    if isinstance(poss_resp, list):\n",
    "                        indicators['indicator_counts']['possible_responses'] += len(poss_resp)\n",
    "                    if isinstance(passkey_req, list):\n",
    "                        indicators['indicator_counts']['passkey_requests'] += len(passkey_req)\n",
    "                    if isinstance(passkey_resp, list):\n",
    "                        indicators['indicator_counts']['passkey_responses'] += len(passkey_resp)\n",
    "                    if url_data.get('passkey_patterns_detected', False):\n",
    "                        indicators['indicator_counts']['pattern_matches'] += 1\n",
    "                \n",
    "                if detect_fedcm_indicators(url_data):\n",
    "                    indicators['fedcm_found'] = True\n",
    "                    fedcm_det = url_data.get('fedcm_detections', [])\n",
    "                    if isinstance(fedcm_det, list):\n",
    "                        indicators['indicator_counts']['fedcm_detections'] += len(fedcm_det)\n",
    "            \n",
    "            domain_results.append({\n",
    "                'domain': domain,\n",
    "                'url_count': len(domain_data),\n",
    "                'classification': {\n",
    "                    'secure_html_only': classification['secure_html_only'],\n",
    "                    'secure_network_only': classification['secure_specific_only'],\n",
    "                    'secure_both': classification['secure_both'],\n",
    "                    'no_secure': classification['no_secure_found'],\n",
    "                    'has_possible': classification['has_possible'],\n",
    "                    'has_fedcm': classification['has_fedcm'],\n",
    "                    'weighted_score': classification['weighted_score']\n",
    "                },\n",
    "                'indicators': indicators\n",
    "            })\n",
    "        \n",
    "        domain_filename = f'{scraper_name}_domain_analysis_{timestamp}.json'\n",
    "        domain_path = os.path.join(EXPORT_DIR, domain_filename)\n",
    "        with open(domain_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(domain_results, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"📊 Domain analysis exported: {domain_path}\")\n",
    "    \n",
    "    # 3. Export standardized metrics summary\n",
    "    if 'metrics' in locals():\n",
    "        metrics_summary = {\n",
    "            'scraper_type': scraper_name,\n",
    "            'analysis_timestamp': timestamp,\n",
    "            'standardized_metrics': metrics,\n",
    "            'timeout_analysis': timeout_analysis if timeout_analysis else [],\n",
    "            'export_info': {\n",
    "                'total_domains_analyzed': len(domain_groups) if domain_groups else 0,\n",
    "                'total_urls_analyzed': len(df) if not df.empty else 0,\n",
    "                'classification_method': 'standardized_domain_based',\n",
    "                'weighted_scoring': True\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        metrics_filename = f'{scraper_name}_metrics_summary_{timestamp}.json'\n",
    "        metrics_path = os.path.join(EXPORT_DIR, metrics_filename)\n",
    "        with open(metrics_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(metrics_summary, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"📈 Metrics summary exported: {metrics_path}\")\n",
    "    \n",
    "    # 4. Export visualizations\n",
    "    if 'visualizations' in locals() and visualizations:\n",
    "        for i, fig in enumerate(visualizations):\n",
    "            if fig:\n",
    "                viz_filename = f'{scraper_name}_standardized_analysis_{timestamp}.png'\n",
    "                viz_path = os.path.join(EXPORT_DIR, viz_filename)\n",
    "                fig.savefig(viz_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "                print(f\"🖼️  Visualization exported: {viz_path}\")\n",
    "    \n",
    "    # 5. Print export summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"✅ NETWORK SCRAPER - STANDARDIZED EXPORT COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"📂 Export Directory: {EXPORT_DIR}\")\n",
    "    print(f\"⏰ Timestamp: {timestamp}\")\n",
    "    print()\n",
    "    print(f\"📊 Standardized Domain Analysis:\")\n",
    "    if 'metrics' in locals():\n",
    "        print(f\"   • Total domains: {metrics['total_domains']}\")\n",
    "        print(f\"   • HTML only (1pt): {metrics['secure_html_only']} domains\")\n",
    "        print(f\"   • Network only (1pt): {metrics['secure_specific_only']} domains\")\n",
    "        print(f\"   • Both indicators (2pts): {metrics['secure_both']} domains\")\n",
    "        print(f\"   • Total secure: {metrics['total_secure']} domains\")\n",
    "        print(f\"   • Weighted score: {metrics['weighted_total']} points\")\n",
    "        print(f\"   • Possible indicators: {metrics['possible_passkey']} domains\")\n",
    "        print(f\"   • FedCM indicators: {metrics['fedcm']} domains\")\n",
    "    \n",
    "    print()\n",
    "    print(\"🎯 Standardization Status:\")\n",
    "    print(\"   ✅ Uses same domain classification logic as other scrapers\")\n",
    "    print(\"   ✅ Weighted scoring system (HTML=1pt, Network=1pt, Both=2pts)\")  \n",
    "    print(\"   ✅ Domain-based evaluation (not URL-based)\")\n",
    "    print(\"   ✅ Consistent visualization style\")\n",
    "    print(\"   ✅ Two-cell notebook structure\")\n",
    "    print()\n",
    "    print(\"📁 Files exported:\")\n",
    "    if not df.empty:\n",
    "        print(f\"   • Raw data: {csv_filename}\")\n",
    "    if domain_groups:\n",
    "        print(f\"   • Domain analysis: {domain_filename}\")\n",
    "    if 'metrics' in locals():\n",
    "        print(f\"   • Metrics summary: {metrics_filename}\")\n",
    "    if 'visualizations' in locals() and visualizations:\n",
    "        print(f\"   • Visualization: {viz_filename}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Export error: {e}\")\n",
    "    print(\"Some exports may be incomplete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
